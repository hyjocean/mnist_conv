# Auto detect text files and perform LF normalization
* text=auto
# -*- coding: utf-8 -*-
"""
Created on Tue Sep 25 20:08:24 2018

@author: BLUE
"""

import input_data
mnist = input_data.read_data_sets("./MNIST_data/", one_hot=True)  #导入数据
import tensorflow as tf
x = tf.placeholder("float",[None,784])          #tf.placeholder(dtype, shape=None, name=None)
                                                #dtype数据类型   shape数据形状[None][None,3]
                                                #name 名称
W = tf.Variable(tf.zeros([784,10]))             #tf.Variable(initializer,name)
                                                #参数initializer是初始化参数，name是可自定义的变量名称
                                                #类似于直接赋予变量值
b = tf.Variable(tf.zeros([10]))
y = tf.nn.softmax(tf.matmul(x,W)+b)             #矩阵乘法A*B
                                                #tf.nn为神经网络缩写。神经网络中对其进行softmax操作
y_ = tf.placeholder("float",[None,10])          #[None,10]列是10，行不定
sess = tf.InteractiveSession()
def weight_variable(shape):                         #定义权重
    initial = tf.truncated_normal(shape,stddev=0.1) #截断的产生正态分布的函数
                                                    #产生正太分布的值如果与均值的差值大于两倍的标准差，那就重新生成。
                                                    #这个函数产生的随机数与均值的差距不会超过两倍的标准差
    return tf.Variable(initial)                     #初始化

def bias_variable(shape):                           #偏置量
    initial = tf.constant(0.1,shape=shape)          #创建一个常数张量，传入list和数值填充
    return tf.Variable(initial)
"权重初始化"
def conv2d(x,W):                                                #定义卷积层
    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')   #神经网络中直接采用conv2d二维卷积
                                                                #设置参数，步长，填充
def max_pool_2x2(x):                                                            #定义最大池化层
    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')   #设置大小，步长。


                    '''卷积和池化'''
"第一层卷积↓"
W_conv1 = weight_variable([5,5,1,32])       #前两个维度是patch的大小， 接着是输入的通道数目，最后是输出的通道数目
b_conv1 = bias_variable([32])

x_image = tf.reshape(x,[-1,28,28,1])        #吧x变成一个4d向量，其2，3维对应图片的宽、高，最后一维代表图片颜色通道数

h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)
#把x_image和权值向量进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max pooling
"第二层卷积↓"
W_conv2 = weight_variable([5,5,32,64])
b_conv2 = bias_variable([64])

h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2) #卷积后得值等于对conv2d relu非线性后的值
h_pool2 = max_pool_2x2(h_conv2)                         #对relu层池化
"密集连接层↓"
W_fc1 = weight_variable([7*7*64,1024])                  #权重值设置
b_fc1 = bias_variable([1024])                           #偏置设置
h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])          #将张量h_pool2转换为一个49*64列的参数
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1) #池化层
#图片尺寸减小到7x7，加入一个有1024个神经元的全连接层
"Dropout"
keep_prob = tf.placeholder("float")             #dropout防止过拟合
h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)
#减少过拟合，用一个placeholder来代表一个神经元的输出 在dropout中保持不变的概率
#我们可以在训练过程中启用dropout，在测试过程中关闭它
"输出层"
W_fc2 = weight_variable([1024,10])
b_fc2 = bias_variable([10])

y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2)
#添加softmax层，

"训练和评估模型"
cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))                   #压缩求和，用于降维
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)   #添加操作以loss通过更新最小化var_list。
correct_prediction = tf.equal(tf.argmax(y_conv,1),tf.argmax(y_,1))  #最终卷积层中最大值索引，和y_最大值索引
#tf.equal(A, B)是对比这两个矩阵或者向量的相等的元素，如果是相等的那就返回True，反正返回False，返回的值的矩阵维度和A是一样的
accuracy = tf.reduce_mean(tf.cast(correct_prediction,"float"))      #将correct_prediction转成float型的数据，在压缩求和降维
sess.run(tf.initialize_all_variables())
for i in range(20000):
    batch = mnist.train.next_batch(50)
    if i%100 ==0:
        train_accuracy = accuracy.eval(feed_dict={
                x:batch[0],y_:batch[1],keep_prob:1.0})
        print("step %d,training accuracy %g"%(i,train_accuracy))
    train_step.run(feed_dict={x:batch[0],y_:batch[1],keep_prob:0.5})

print("test accuracy %g"%accuracy.eval(feed_dict={
        x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0}))
